{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import scipy.signal as signal\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT_IDS = [i for i in range(2, 18) if i != 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:06<00:00, 12.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Loaded All Subject Info --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [02:01<05:34, 30.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m wrist_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mACC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBVP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEDA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEMP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject_id \u001b[38;5;129;01min\u001b[39;00m tqdm(SUBJECT_IDS):\n\u001b[0;32m---> 30\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWESAD/S\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubject_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/S\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubject_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Load in all chest information\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     chest_info \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data1030_env/lib/python3.12/site-packages/joblib/numpy_pickle.py:651\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 651\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_read_fileobject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# if the returned file object is a string, this means we\u001b[39;49;00m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# try to load a pickle file generated with an version of\u001b[39;49;00m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Joblib so we load it with joblib compatibility function.\u001b[39;49;00m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data1030_env/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data1030_env/lib/python3.12/site-packages/joblib/numpy_pickle_utils.py:149\u001b[0m, in \u001b[0;36m_read_fileobject\u001b[0;34m(fileobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility function opening the right fileobject from a filename.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mThe magic number is used to choose between the type of file object to open:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Detect if the fileobj contains compressed data.\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m compressor \u001b[38;5;241m=\u001b[39m \u001b[43m_detect_compressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compressor \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompat\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Compatibility with old pickle mode: simply return the input\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# filename \"as-is\" and let the compatibility function be called by the\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# caller.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a joblib \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    158\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data1030_env/lib/python3.12/site-packages/joblib/numpy_pickle_utils.py:94\u001b[0m, in \u001b[0;36m_detect_compressor\u001b[0;34m(fileobj)\u001b[0m\n\u001b[1;32m     90\u001b[0m max_prefix_len \u001b[38;5;241m=\u001b[39m _get_prefixes_max_len()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeek\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Peek allows to read those bytes without moving the cursor in the\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# file whic.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     first_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeek\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_prefix_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Fallback to seek if the fileobject is not peekable.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     first_bytes \u001b[38;5;241m=\u001b[39m fileobj\u001b[38;5;241m.\u001b[39mread(max_prefix_len)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LOAD ALL DATA\n",
    "subject_cols = ['age', 'height_cm', 'weight_kg', 'gender', 'is_smoker']\n",
    "subject_info = pd.DataFrame(columns = subject_cols)\n",
    "\n",
    "for subject_id in tqdm(SUBJECT_IDS):\n",
    "    with open(f'WESAD/S{subject_id}/S{subject_id}_readme.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        age = int(lines[1].split(\":\")[1][1:-1])\n",
    "        height_cm = int(lines[2].split(\":\")[1][1:-1])\n",
    "        weight_kg = int(lines[3].split(\":\")[1][1:-1])\n",
    "        gender = lines[4].split(\":\")[1][1:-1]\n",
    "        is_smoker = lines[10].split(\"?\")[1][1:-1]\n",
    "    \n",
    "        info = pd.DataFrame([[age, height_cm, weight_kg, gender, is_smoker]], columns = subject_cols)\n",
    "\n",
    "        subject_info = pd.concat([subject_info, info], axis=0)\n",
    "\n",
    "subject_info.index = SUBJECT_IDS\n",
    "\n",
    "print('-- Loaded All Subject Info --')\n",
    "\n",
    "subject_chest_data: dict[int, pd.DataFrame] = dict()\n",
    "subject_wrist_data: dict[int, pd.DataFrame] = dict()\n",
    "\n",
    "chest_features = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "wrist_features = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "\n",
    "for subject_id in tqdm(SUBJECT_IDS):\n",
    "    data = joblib.load(f'WESAD/S{subject_id}/S{subject_id}.joblib')\n",
    "\n",
    "    # Load in all chest information\n",
    "    chest_info = data['signal']['chest']\n",
    "    chest_data = {feature: chest_info[feature] for feature in chest_features}\n",
    "\n",
    "    chest_dict = {\n",
    "        'ACC_X': chest_data['ACC'][:, 0],\n",
    "        'ACC_Y': chest_data['ACC'][:, 1],\n",
    "        'ACC_Z': chest_data['ACC'][:, 2],\n",
    "        'ECG': chest_data['ECG'].flatten(),\n",
    "        'EMG': chest_data['EMG'].flatten(),\n",
    "        'EDA': chest_data['EDA'].flatten(),\n",
    "        'TEMP': chest_data['Temp'].flatten(),\n",
    "        'RESP': chest_data['Resp'].flatten(),\n",
    "        'STATE': data['label'].flatten()\n",
    "    }\n",
    "\n",
    "    chest_df = pd.DataFrame(chest_dict)\n",
    "    subject_chest_data[subject_id] = chest_df\n",
    "\n",
    "    # Load in all wrist information\n",
    "    wrist_info = data['signal']['wrist']\n",
    "    wrist_data = {feature: wrist_info[feature] for feature in wrist_features}\n",
    "\n",
    "    wrist_dict = {\n",
    "        'ACC_X': signal.decimate(wrist_data['ACC'][:, 0], 8),\n",
    "        'ACC_Y': signal.decimate(wrist_data['ACC'][:, 1], 8),\n",
    "        'ACC_Z': signal.decimate(wrist_data['ACC'][:, 2], 8),\n",
    "        'BVP': signal.decimate(wrist_data['BVP'].flatten(), 16),\n",
    "        'EDA': wrist_data['EDA'].flatten(),\n",
    "        'TEMP': wrist_data['TEMP'].flatten(),\n",
    "        'STATE': data['label'][::175]\n",
    "    }\n",
    "\n",
    "    wrist_df = pd.DataFrame(wrist_dict)\n",
    "\n",
    "    total_labels = Counter(data['label'])\n",
    "    total_labels.update({i: 0 for i in range(8)})\n",
    "    assert np.allclose((pd.Series(total_labels) / wrist_df['STATE'].value_counts()).fillna(175).to_numpy(), np.full(8, 175), atol = 1e-2)\n",
    "    \n",
    "    subject_wrist_data[subject_id] = wrist_df\n",
    "\n",
    "print('-- Loaded All Wrist & Chest Sensor Info --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-EDA PREPROCESSING\n",
    "\n",
    "# From all dataframes in chest and wrist data, remove rows with labels 5, 6, 7 (ignored as demanded by experiment)\n",
    "for subject_id in subject_chest_data.keys():\n",
    "    subject_chest_data[subject_id] = subject_chest_data[subject_id][~subject_chest_data[subject_id]['STATE'].isin([5, 6, 7])]\n",
    "    subject_chest_data[subject_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "for subject_id in subject_wrist_data.keys():\n",
    "    subject_wrist_data[subject_id] = subject_wrist_data[subject_id][~subject_wrist_data[subject_id]['STATE'].isin([5, 6, 7])]\n",
    "    subject_wrist_data[subject_id].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-EDA PREPROCESSING\n",
    "\n",
    "# Add to each dataframe (chest and wrist) a column that is that subject's ID\n",
    "for subject_id in subject_chest_data.keys():\n",
    "    subject_chest_data[subject_id] = subject_chest_data[subject_id].copy()\n",
    "    subject_chest_data[subject_id].loc[:, 'SUBJECT_ID'] = subject_id\n",
    "\n",
    "for subject_id in subject_wrist_data.keys():\n",
    "    subject_wrist_data[subject_id] = subject_wrist_data[subject_id].copy()\n",
    "    subject_wrist_data[subject_id].loc[:, 'SUBJECT_ID'] = subject_id\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "all_chest_data = pd.concat(subject_chest_data.values(), axis=0)\n",
    "all_wrist_data = pd.concat(subject_wrist_data.values(), axis=0)\n",
    "all_chest_data.reset_index(drop=True, inplace=True)\n",
    "all_wrist_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Chest Size: (47300400, 10)\n",
      "Val Chest Size: (5912550, 10)\n",
      "Test Chest Size: (5912550, 10)\n",
      "Train Wrist Size: (202716, 8)\n",
      "Val Wrist Size: (67572, 8)\n",
      "Test Wrist Size: (67572, 8)\n"
     ]
    }
   ],
   "source": [
    "# Must stratify on 75 distinct categories (5 States, 15 Subjects) to ensure equal representation in train/val/test\n",
    "chest_train_perc, chest_val_perc, chest_test_perc = 0.8, 0.1, 0.1\n",
    "wrist_train_perc, wrist_val_perc, wrist_test_perc = 0.6, 0.2, 0.2\n",
    "\n",
    "all_chest_data['STATE_SUBJECT'] = all_chest_data['STATE'].astype(str) + '_' + all_chest_data['SUBJECT_ID'].astype(str)\n",
    "all_wrist_data['STATE_SUBJECT'] = all_wrist_data['STATE'].astype(str) + '_' + all_wrist_data['SUBJECT_ID'].astype(str)\n",
    "\n",
    "chest_X_train, chest_X_other, chest_y_train, chest_y_other = train_test_split(all_chest_data, all_chest_data['STATE'], train_size = chest_train_perc, stratify = all_chest_data['STATE_SUBJECT'], random_state = 42)\n",
    "chest_X_val, chest_X_test, chest_y_val, chest_y_test = train_test_split(chest_X_other, chest_y_other, test_size = (chest_test_perc / (chest_val_perc + chest_test_perc)), stratify = chest_X_other['STATE_SUBJECT'], random_state = 42)\n",
    "\n",
    "wrist_X_train, wrist_X_other, wrist_y_train, wrist_y_other = train_test_split(all_wrist_data, all_wrist_data['STATE'], train_size = wrist_train_perc, stratify = all_wrist_data['STATE_SUBJECT'], random_state = 42)\n",
    "wrist_X_val, wrist_X_test, wrist_y_val, wrist_y_test = train_test_split(wrist_X_other, wrist_y_other, test_size = (wrist_test_perc / (wrist_val_perc + wrist_test_perc)), stratify = wrist_X_other['STATE_SUBJECT'], random_state = 42)\n",
    "\n",
    "# Drop the STATE_SUBJECT columns and reset indices\n",
    "chest_X_train = chest_X_train.drop(columns = ['STATE_SUBJECT']).reset_index(drop=True)\n",
    "chest_X_val = chest_X_val.drop(columns = ['STATE_SUBJECT']).reset_index(drop=True)\n",
    "chest_X_test = chest_X_test.drop(columns = ['STATE_SUBJECT']).reset_index(drop=True)\n",
    "\n",
    "wrist_X_train = wrist_X_train.drop(columns = ['STATE_SUBJECT']).reset_index(drop=True)\n",
    "wrist_X_val = wrist_X_val.drop(columns = ['STATE_SUBJECT']).reset_index(drop=True)\n",
    "wrist_X_test = wrist_X_test.drop(columns = ['STATE_SUBJECT']).reset_index(drop=True)\n",
    "\n",
    "print('Train Chest Size:', chest_X_train.shape)\n",
    "print('Val Chest Size:', chest_X_val.shape)\n",
    "print('Test Chest Size:', chest_X_test.shape)\n",
    "\n",
    "print('Train Wrist Size:', wrist_X_train.shape)\n",
    "print('Val Wrist Size:', wrist_X_val.shape)\n",
    "print('Test Wrist Size:', wrist_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify overarching split proportion sizes\n",
    "train_chest_subjects = chest_X_train['SUBJECT_ID'].value_counts()\n",
    "val_chest_subjects = chest_X_val['SUBJECT_ID'].value_counts()\n",
    "test_chest_subjects = chest_X_test['SUBJECT_ID'].value_counts()\n",
    "\n",
    "train_wrist_subjects = wrist_X_train['SUBJECT_ID'].value_counts()\n",
    "val_wrist_subjects = wrist_X_val['SUBJECT_ID'].value_counts()\n",
    "test_wrist_subjects = wrist_X_test['SUBJECT_ID'].value_counts()\n",
    "\n",
    "assert np.allclose(train_chest_subjects / (train_chest_subjects + val_chest_subjects + test_chest_subjects), chest_train_perc, atol = 1e-2)\n",
    "assert np.allclose(val_chest_subjects / (train_chest_subjects + val_chest_subjects + test_chest_subjects), chest_val_perc, atol = 1e-2)\n",
    "assert np.allclose(test_chest_subjects / (train_chest_subjects + val_chest_subjects + test_chest_subjects), chest_test_perc, atol = 1e-2)\n",
    "\n",
    "assert np.allclose(train_wrist_subjects / (train_wrist_subjects + val_wrist_subjects + test_wrist_subjects), wrist_train_perc, atol = 1e-2)\n",
    "assert np.allclose(val_wrist_subjects / (train_wrist_subjects + val_wrist_subjects + test_wrist_subjects), wrist_val_perc, atol = 1e-2)\n",
    "assert np.allclose(test_wrist_subjects / (train_wrist_subjects + val_wrist_subjects + test_wrist_subjects), wrist_test_perc, atol = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder on SUBJECT_ID \n",
    "chest_subject_ohe_encoder = OneHotEncoder(sparse_output = False, handle_unknown = 'error')\n",
    "wrist_subject_ohe_encoder = OneHotEncoder(sparse_output = False, handle_unknown = 'error')\n",
    "\n",
    "oh_features = ['SUBJECT_ID']\n",
    "\n",
    "chest_X_train_ohe = chest_subject_ohe_encoder.fit_transform(chest_X_train[oh_features])\n",
    "chest_X_val_ohe = chest_subject_ohe_encoder.transform(chest_X_val[oh_features])\n",
    "chest_X_test_ohe = chest_subject_ohe_encoder.transform(chest_X_test[oh_features])\n",
    "\n",
    "wrist_X_train_ohe = wrist_subject_ohe_encoder.fit_transform(wrist_X_train[oh_features])\n",
    "wrist_X_val_ohe = wrist_subject_ohe_encoder.transform(wrist_X_val[oh_features])\n",
    "wrist_X_test_ohe = wrist_subject_ohe_encoder.transform(wrist_X_test[oh_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler on all non-target features\n",
    "chest_subject_ss_encoder = StandardScaler()\n",
    "wrist_subject_ss_encoder = StandardScaler()\n",
    "\n",
    "chest_ss_features = ['ACC_X', 'ACC_Y', 'ACC_Z', 'ECG', 'EMG', 'EDA', 'TEMP', 'RESP']\n",
    "wrist_ss_features = ['ACC_X', 'ACC_Y', 'ACC_Z', 'BVP', 'EDA', 'TEMP']\n",
    "\n",
    "chest_X_train_ss = chest_subject_ss_encoder.fit_transform(chest_X_train[chest_ss_features])\n",
    "chest_X_val_ss = chest_subject_ss_encoder.transform(chest_X_val[chest_ss_features])\n",
    "chest_X_test_ss = chest_subject_ss_encoder.transform(chest_X_test[chest_ss_features])\n",
    "\n",
    "wrist_X_train_ss = wrist_subject_ss_encoder.fit_transform(wrist_X_train[wrist_ss_features])\n",
    "wrist_X_val_ss = wrist_subject_ss_encoder.transform(wrist_X_val[wrist_ss_features])\n",
    "wrist_X_test_ss = wrist_subject_ss_encoder.transform(wrist_X_test[wrist_ss_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Concatenate all (transformed) features back together and back to DataFrame form\n",
    "\n",
    "chest_X_train = pd.DataFrame(np.concatenate([chest_X_train_ohe, chest_X_train_ss], axis=1), columns = [f'IS_S{SUBJECT_IDS[i]}' for i in range(len(SUBJECT_IDS))] + chest_ss_features)\n",
    "chest_X_val = pd.DataFrame(np.concatenate([chest_X_val_ohe, chest_X_val_ss], axis=1), columns = [f'IS_S{SUBJECT_IDS[i]}' for i in range(len(SUBJECT_IDS))] + chest_ss_features)\n",
    "chest_X_test = pd.DataFrame(np.concatenate([chest_X_test_ohe, chest_X_test_ss], axis=1), columns = [f'IS_S{SUBJECT_IDS[i]}' for i in range(len(SUBJECT_IDS))] + chest_ss_features)\n",
    "\n",
    "wrist_X_train = pd.DataFrame(np.concatenate([wrist_X_train_ohe, wrist_X_train_ss], axis=1), columns = [f'IS_S{SUBJECT_IDS[i]}' for i in range(len(SUBJECT_IDS))] + wrist_ss_features)\n",
    "wrist_X_val = pd.DataFrame(np.concatenate([wrist_X_val_ohe, wrist_X_val_ss], axis=1), columns = [f'IS_S{SUBJECT_IDS[i]}' for i in range(len(SUBJECT_IDS))] + wrist_ss_features)\n",
    "wrist_X_test = pd.DataFrame(np.concatenate([wrist_X_test_ohe, wrist_X_test_ss], axis=1), columns = [f'IS_S{SUBJECT_IDS[i]}' for i in range(len(SUBJECT_IDS))] + wrist_ss_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=23, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(chest_X_train.columns)\n",
    "print(chest_X_train.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
